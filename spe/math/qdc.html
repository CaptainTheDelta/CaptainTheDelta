<!DOCTYPE html>
<html lang="fr">
  <head>
    <title>Questions de cours</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../../styles/style.css">
    <link rel="stylesheet" href="../../styles/smartphone.css">
    <!-- JS pour le LaTeX -->
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js' async></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        jax: ["input/TeX","output/CommonHTML"],
        extensions: ["tex2jax.js"],
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        },
        CommonHTML: {
          minScaleAdjust: 90
        },
        TeX: {
          extensions: ["noErrors.js","noUndefined.js"]
        }
      });
    </script>
  </head>
  <body>
  <h1 class="sred">Analyse</h1>
  <div>
    <h3 class="sblue">Sommes de Riemann</h3>
    <p>
      La fonction $f$ étant continue sur un segment $[a,b]$, on se propose de déterminer une valeur approchée de l'intégrale $\int_a^bf(x)dx$.
    </p>
    <p>
      En subdivisant l'intervalle en n sous-intervalles $[a_i,a_{i+1}]$ de même amplitude $(b-a)/n$:<br>
      On définit deux sommes de Riemann de $f$ sur $[a,b]$: 
      $$S_n       = \frac{b-a}{n}\cdot\sum_{k=0}^{n-1}\ f\left[a + \frac{k(b-a)}{n}\right]$$
      $$\Sigma_n  = \frac{b-a}{n}\cdot\sum_{k=1}^{n}  \ f\left[a + \frac{k(b-a)}{n}\right]$$
    </p>
  </div>

  <div>
      <h3 class="sblue">Développements en séries entières classiques avec leurs rayons de convergence</h3>
      
      <p>
        <table>
          <tr>
            <td>$$\frac{1}{1+x}$$</td>
            <td>$$\sum_{p=0}^{+\infty} (-1)^p x^p$$</td>
            <td>$$R=1$$</td>
          </tr>
          <tr>
            <td>$$\ln(1+x)$$</td>
            <td>$$\sum_{p=1}^{+\infty} (-1)^{p+1} \frac{x^p}{p}$$</td>
            <td>$$R=1$$</td>
          </tr>
          <tr>
            <td>$$e^x$$</td>
            <td>$$\sum_{p=0}^{+\infty} (-1)^p x^p$$</td>
            <td>$$R=+\infty$$</td>
          </tr>
          <tr>
            <td>$$ch(x)$$</td>
            <td>$$\sum_{p=0}^{+\infty} \frac{x^{2p}}{(2p)!}$$</td>
            <td>$$R=+\infty$$</td>
          </tr>
          <tr>
            <td>$$sh(x)$$</td>
            <td>$$\sum_{p=0}^{+\infty} \frac{x^{2p+1}}{(2p+1)!}$$</td>
            <td>$$R=+\infty$$</td>
          </tr>
          <tr>
            <td>$$cos(x)$$</td>
            <td>$$\sum_{p=0}^{+\infty} (-1)^p \frac{x^{2p}}{(2p)!}$$</td>
            <td>$$R=+\infty$$</td>
          </tr>
          <tr>
            <td>$$sin(x)$$</td>
            <td>$$\sum_{p=0}^{+\infty} (-1)^p \frac{x^{2p+1}}{(2p+1)!}$$</td>
            <td>$$R=+\infty$$</td>
          </tr>
        </table>
        <p>
          $$(1+x)^\alpha=1 + \sum_{p=1}^{+\infty} \frac{\alpha(\alpha-1)...(\alpha-n+1)}{p!} x^p$$
          avec $R=1$
        </p>
      </p>
      
  </div>

  <hr>
  <h1 class="sred">Probabilités</h1>
  <div>
    <h3 class="sblue">Inégalités de Markov et de Bienaymé-Tchebychev</h3>
    <ul>
      <li>Markov</li>
    </ul>
    <p>Soit $X$ une variable aléatoire discrète.</p>
    <p>
      Si $E[X]$ est finie, pour tout réel $a$ strictement positif,
      $$P(|X|\ge a) \le \frac{1}{a} E[|X|]$$
    </p>
    <p>
      Si $E[X^2]$ est finie, pour tout réel $a$ strictement positif,
      $$P(|X|\ge a) \le \frac{1}{a^2} E[X^2]$$
    </p>
    <ul>
      <li class="sblue">Bienaymé-Tchebychev</li>
    </ul>
    <p>Soit $X$ une variable aléatoire discrète telle que $X^2$ ait une espérance finie.</p>
    <p>
      Alors, pour tout réel $a$ strictement positif,
      $$P\left(\left|\frac{X-E[X]}{\sigma(X)}\right|\ge a\right) \le \frac{1}{a^2}$$
    </p>
  </div>

  <div>
    <h3 class="sblue">Loi faible des grands nombres</h3>
    <p>
      Soit $(X_n)_{n \ge 1}$ une suite de variables aléatoires discrètes, indépendantes deux à deux et de même loi (ou équidistribuées) telles que $X_n^2$ soit d'espérance finie.
    </p>
    <p>
      Alors, en notant $m = E[X_n]$, $\sigma = \sigma(X_n)$ et $S_n = (X_1 + ... + X_n)/n$, on a pour tout réel $\varepsilon$ strictement positif,
      $$P(|S_n - m| \ge \varepsilon) \le \frac{\sigma^2}{n\varepsilon^2}$$
      Par suite,
      $$\lim_{n \to +\infty} P(|S_n - m| \ge \varepsilon) = 0$$
    </p>
    <p>
      On dit que la suite des moyennes arithmétiques $(S_n)_{n \ge 1}$ converge en probabilité vers $m$.
    </p>
    <p>
      Concrètement, cela signifie que pour $\varepsilon > 0$, la probabilité pour que la moyenne arithmétique s'écarte de $m$ de plus de $\varepsilon$ est pratiquement nulle si $n$ assez grand.
    </p>
    <p>
      Si on répète un grand nombre de fois une expérience correspondant à la variable aléatoire $X$, la moyenne empirique $S_n$ permet d'estimer en probabilité la moyenne de $X$.
    </p>
    <p></p>
  </div>
  <hr>

  <h1 class="sred">Algèbre-Géométrie</h1>
    <div>
      <h3 class="sblue">Définition du produit scalaire</h3>
      <p>
        Soit $E$ un espace vectoriel sur $\mathbb R$. Un produit scalaire sur $E$ est une forme bilinéaire $\varphi$ sur $E$ qui est symétrique, définie et postitive,<br>
        c'est à dire, $\varphi$ est une application de $E^2$ dans $\mathbb R$ qui est:
      </p>
      <ul>
        <li>symétrique : $\varphi(\vec u,\vec v) = \varphi(\vec v, \vec u)$</li>
        <li>bilinéaire: $\varphi(\vec{u_1} + \lambda \vec{u_2},\vec v) = \varphi(\vec{u_1},\vec v) + \lambda\varphi(\vec{u_2},\vec v)$</li>
        <li>positive: $\varphi(\vec u,\vec u) \ge 0$</li>
        <li>définie: $\varphi(\vec u,\vec u) = 0 \Rightarrow \vec u = \vec 0$</li>
      </ul>
    </div>
    <br>
    <div>
      <h3 class="sblue">Produits scalaires canoniques, exemples</h3>
      <p>
        Soient $a$ et $b$ deux réels distincts. L'application 
        $$\phi : (f,g) \mapsto \int_a^b f(x)g(x)dx$$
        défini un produit scalaire sur l'espace vectoriel $\mathcal C([a,b],\mathbb R)$.
      </p>
      <p>
        Sur $\mathbb R^n$, si $x=(x_1,...,x_n)$ et $y=(y_1,...,y_n)$ alors
        $$\langle x,y \rangle = \sum_{i=1}^n x_i\ y_i$$
      </p>
      <p>
        Sur $\mathbb R_n[X]$, si $P = \sum_{i=0}^n a_i X^i$ et $Q = \sum_{i=0}^n b_i X^i$ alors
        $$\langle P,Q \rangle = \sum_{i=0}^n a_i\ b_i$$
      </p>
      <p>
        Sur $\mathcal M_n(\mathbb R)$, si $A = (a_{ij})_{1\le i,j \le n}$ et $B = (a_{ij})_{1\le i,j \le n}$ alors
        $$\langle A,B \rangle = \sum_{i=1}^n\sum_{j=1}^n a_{ij}\ b_{ij} = \text{tr}(^tAB)$$
      </p>
    </div>

    <div>
      <h3 class="sblue">Procédé d'orthonormalisation de Gram-Schmidt</h3>
      <ul>
        <li>Théorème</li>
      </ul>
      <p>
        Si $(\vec e_n)_{n \ge 1}$ est une famille libre d'un espace préhilbertien $E$, il existe une unique famille orthonormée $(\vec e_n')_{n \ge 1}$ telle que, pour tout $k \ge 1$,
        $$Vect(\vec e_1,...,\vec e_k)=Vect(\vec e_1',...,\vec e_k')$$
        et 
        $$\langle \vec e_k, \vec e_k' \rangle > 0$$ 
      </p>
      <div class="meth">
        <ul>
          <li>Procédé</li>
        </ul>
        <p>
          Algorithme de construction dans un espace préhilbertien d'une famille orthonormée $(\overrightarrow e_n')_{n \ge 1}$ à partir d'une famille libre $(\vec e_n)_{n \ge 1}$.
        </p>
        <p>
          Pour cela, on montre par récurrence la proposition $\mathcal P_k$ suivante :<br>
          Il existe une famille orthonormée de vecteurs $(\vec e_1',...,\vec e_k')$ telle que $Vect(\vec e_1,...,\vec e_k)=Vect(\vec e_1',...,\vec e_k')$ et $\langle \vec e_k, \vec e_k' \rangle > 0$.
        </p>
        <p>
          On fonde la récurrence en posant $\vec e_1' = \vec e_1/\|\vec e_1\|$ vecteur unitaire tel que :
          $$\langle \vec e_1',\vec e_1\rangle = \|\vec e_1\| > 0$$
        </p>
        <p>
          On suppose $\mathcal P_k$ vraie, on pose alors :
          $$\vec u_{k+1}' = \vec e_{k+1} + \lambda_1\vec e_{1}'+...+\lambda_k\vec e_{k}'$$
          puis
          $$\vec e_{k+1}' = \frac{\vec u_{k+1}'}{\|\vec u_{k+1}'\|}$$
          et on détermine les réels $\lambda_1,...,\lambda_{k}$ de sorte que $\vec e_{k+1}'$ soit orthogonal à chaque $\vec e_{i}'$, pour $i$ allant de $1$ à $k$, c'est à dire, puisque $\mathcal P_k$ est vraie,
          $$\langle \vec e_{k+1}',\vec e_{i}'\rangle = 0 = \langle\vec e_{k+1},\vec e_{i}'\rangle + \lambda_i$$
          on en déduit,
          $$\lambda_i = -\langle\vec e_{k+1},\vec e_{i}'\rangle$$
          De plus, $Vect(\vec e_1,...,\vec e_k)=Vect(\vec e_1',...,\vec e_k')$ donc $\vec u_{k+1}' \neq 0$ et $\vec e_{k+1}' \in Vect(\vec e_1,...,\vec e_{k+1})$. Donc
          $$Vect(\vec e_1,...,\vec e_{k+1}) \subset Vect(\vec e_1',...,\vec e_{k+1}')$$
          De plus, par construction, $\vec e_{k+1}' \in Vect(\vec e_1',...,\vec e_{k+1}')$ et par hypothèse de récurrence, $\vec e_1,...,\vec e_k$ sont combinaisons linéaires de $\vec e_1',...,\vec e_k'$. Donc
          $$Vect(\vec e_1',...,\vec e_{k+1}') \subset Vect(\vec e_1,...,\vec e_{k+1})$$
          Enfin,
          $$1=\|\vec e_{k+1}'\|^2 = \left\langle\frac{\vec u_{k+1}'}{\|\vec u_{k+1}'\|}, \vec e_{k+1}'\right\rangle$$
          d'où
          $$\|\vec u_{k+1}'\| = \langle\vec e_{k+1},\vec e_{k+1}'\rangle > 0$$
          On a terminé la récurrence en prouvant que  $\mathcal P_{k+1}$ est vraie.
        </p>
      </div>
      <ul>
        <li>Poème</li>
      </ul>
      <div class="poem">
        <pre>
Envie d'ordre, de paix, d'angles droits, de corps sages ?
Ne seriez-vous pas las de ces bases sauvages ?
Oyez, braves matheux, sortez de vos guérites,
Écoutez les conseils de messieurs Gram et Schmidt !

Divisez par sa norme un vecteur esseulé
Si vous en avez n, pensez à morceler :
Supposons qu'n-1 vous soient déjà plaisants,
trouvez le dernier-né et saisissez vous-en !

Retranchez-lui ce qui, chez ses frères, lui pèse ;
Frais, libre, orthogonal et revitalisé,
Il ne vous reste plus qu'à le normaliser.

Ainsi par récurrence on poursuit la genèse.
Plus fort que Vandermonde et plus puissant qu'Hermite,
C'est l'humble procédé de messieurs Gram et Schmidt !

Guillaume Dubach
        </pre>
      </div>
    </div>

    <div>
      <h3 class="sblue">Inégalité de Cauchy-Schwarz, de Minkovsky</h3>
      <ul>
        <li>Cauchy-Schwarz</li>
      </ul>
      <p>
        Pour tous vecteurs $\vec u$ et $\vec v$ d'un espace préhilbertien $E$,
        $$|\langle \vec u, \vec v \rangle| \le \|\vec u\|\cdot\|\vec v\|$$
        avec égalité si et seulement si les deux vecteurs $\vec u$ et $\vec v$ sont colinéaires.
      </p>
      <ul>
        <li>Minkovsky</li>
      </ul>
      <p>
        Pour tous vecteurs $\vec u$ et $\vec v$ d'un espace préhilbertien $E$,
        $$\|\vec u + \vec v\| \le \|\vec u\|+\|\vec v\|$$
        avec égalité si et seulement si les deux vecteurs $\vec u$ et $\vec v$ sont colinéaires et de même sens.
      </p>
    </div>

  </body>

  <footer>
    <br>
    <hr>
    <div>
      <p>
        Revenir à l'<a href="../../index.html">index</a>.
      </p>
    </div>
  </footer>
</html> 